{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing A Benchmark Model\n",
    "\n",
    "Having performed a preliminary analysis of the data, we move onto preparing a benchmark model. First, we will do some analysis of the target column. Based upon this analysis we will think about what the best model for a benchmark might be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### load the dataset using `read.csv()`\n",
    "\n",
    "Load the csv file `titanic-updated.csv` into a dataframe called `titanic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c91090591b8274733b454a8e1b3939c6",
     "grade": false,
     "grade_id": "cell-f9ff9561629d8da1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aeb6e9bfcd9618cd6df53cd90d210dae",
     "grade": true,
     "grade_id": "cell-e7a3502ffc2aeedc",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stopifnot(dim(titanic) == c(891,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contingency Tables\n",
    "\n",
    "We will make use of the R `table()` function to study the target column. This function builds a **contingency table** of the counts combinations of factor levels. Of course if only a Single column is passed to the function, it will just return a simple count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define a contingency table of `titanic$survived`\n",
    "\n",
    "Define a table called `table_survival` which is a contingency table for `titanic$survived`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_survival = table(titanic$Survived)\n",
    "table_survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result returns, we can see that the  survival status is stored as either a 0, corresponding to \"did not survive\", or a 1 corresponding to \"survived\". We can use the helper function `prop.table()` to express the results a contingency table as fractions. Here, we can see that approximately $0.6$ of the passengers did not survive. One thing we should immediately take note of is that our target column is not evenly distributed. An **evenly distributed** target column would have the exact same number of each possible outcome. As we grow in our data science practice we will learn more about dealing with an evenly distributed target. For now it is sufficient to simply take note of this fact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define a proportion table of `titanic$survived`\n",
    "\n",
    "Define a table called `prop_table_survival` which is a contingency table for `titanic$survived`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_table_survival = prop.table(table(titanic$Survived))\n",
    "prop_table_survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use a histogram to show once more that the target is not evenly distributed. By default, the `hist()` function simply shows the counts for each measured value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### display a histogram of `titanic$Survived`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(repr)\n",
    "options(repr.plot.width=10, repr.plot.height=4)\n",
    "\n",
    "hist(titanic$Survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Naïve Guess\n",
    "\n",
    "We will use a naive guess based on the most common class as a benchmark. 61.6% of passengers did not survive. We will guess for our benchmark that there were no survivors. Note that we have done very little work and already have a better then 50-50 chance I've getting a correct answer simply by guessing that no one survived. This is one consideration for having an unevenly distributed target. Simply measuring accuracy may not give us a realistic sense of how well our model is doing. This is one reason why preparing a benchmark is so important. Had we not prepared at benchmark we might think that a 55% accuracy is deceny because it's better than the simple 50-50. This benchmark gives us a sense of what we need to do better than in order to prepare a model that adds value to the situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### store the number of passengers\n",
    "\n",
    "Store the number of passengers as the variable `number_of_passengers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9fb4c05a584302341c67efbebe6957bd",
     "grade": false,
     "grade_id": "cell-a37f4ee22df427f2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e7ab7da2168a1747e646e364a31dac54",
     "grade": true,
     "grade_id": "cell-f43ed4236599af78",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### HIDDEN TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a vector called `no_survivors` that is a list of predictions that no one survived.\n",
    "\n",
    "To create such a vector using R, we will use the replicate `rep()` function. This function takes a value and replicates it a given number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_survivors <- rep(0, number_of_passengers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FREE RESPONSE\n",
    "\n",
    "Describe in words the vector `no_survivors`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0623f3dbd98cbb3335b6c6914655166e",
     "grade": true,
     "grade_id": "cell-67b469cc2c4a53ad",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have prepared this naïve guess, we can use the `accuracy` function we defined earlier to assess our benchmark as a vector of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define accuracy metric\n",
    "\n",
    "Write a function called  `accuracy`. This function should takes two vectors as argument: \n",
    "\n",
    "1. a vector of actual values\n",
    "1. a vector of predicted values \n",
    "\n",
    "The function should do two things:\n",
    "\n",
    "1. it should use `verify_length` to make sure that the vectors have the same length.\n",
    "1. it computes the accuracy of a prediction vector where accuracy is defined by\n",
    "\n",
    "$$\\text{accuracy} = \\sum \\left(\\text{class}_{actual} = \\text{class}_{predicted}\\right)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "51556083ce1e66d1bc1f3a9048d56311",
     "grade": false,
     "grade_id": "cell-f0a75dd80bc38a04",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "verify_length <- function (v1, v2 ){\n",
    "    if (length(v1) != length(v2)) {\n",
    "        stop('length of vectors do not match') \n",
    "    }\n",
    "}\n",
    "\n",
    "accuracy <- function (actual, predicted) {\n",
    "    # Write your code here.\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "469118c1d41d7d05dd35ae22cba70de5",
     "grade": true,
     "grade_id": "cell-036501c760d16a7e",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "accuracy(titanic$Survived, no_survivors)\n",
    "stopifnot(round(accuracy(titanic$Survived, no_survivors), 2) == 0.62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If implemented correctly, you should see an accuracy of $0.\\bar{61}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Model\n",
    "\n",
    "This solution, that is simply guessing that no on survived should be taken as a benchmark model. Any subsequent model that we test on this task should achieve an accuracy higher than this or it should be immediately rejected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
